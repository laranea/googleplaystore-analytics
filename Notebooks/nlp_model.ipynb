{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# For Tokenizing\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "from spellchecker import SpellChecker\n",
    "from contractions import contractions_dict\n",
    "\n",
    "from nltk import download, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For Text Classification\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Necessary Corpus for tokenizer, and sentiment analyzers to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lyqht/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/lyqht/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/lyqht/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('punkt')\n",
    "download('stopwords')\n",
    "download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset and rename some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Sentiment_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26969</td>\n",
       "      <td>Clash Royale</td>\n",
       "      <td>I really like meet new people talk friends. It...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1507</td>\n",
       "      <td>8 Ball Pool</td>\n",
       "      <td>Loved game UNTIL I update. Now open even allow...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.009821</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22854</td>\n",
       "      <td>Camera360 Lite - Selfie Camera</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60142</td>\n",
       "      <td>Harry Potter: Hogwarts Mystery</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  App  \\\n",
       "26969                    Clash Royale   \n",
       "1507                      8 Ball Pool   \n",
       "22854  Camera360 Lite - Selfie Camera   \n",
       "60142  Harry Potter: Hogwarts Mystery   \n",
       "\n",
       "                                                  Review Sentiment  \\\n",
       "26969  I really like meet new people talk friends. It...  Positive   \n",
       "1507   Loved game UNTIL I update. Now open even allow...  Negative   \n",
       "22854                                                NaN       NaN   \n",
       "60142                                                NaN       NaN   \n",
       "\n",
       "       Sentiment_Polarity  Sentiment_Subjectivity  \n",
       "26969            0.007273                0.490909  \n",
       "1507            -0.009821                0.607143  \n",
       "22854                 NaN                     NaN  \n",
       "60142                 NaN                     NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/googleplaystore_user_reviews.csv\")\n",
    "df.rename(columns = {'Translated_Review':'Review'}, inplace=True)\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "Source of definitions: Bing Liu. Sentiment Analysis and Opinion Mining, Morgan & Claypool Publishers, May 2012\n",
    "- Sentiment Polarity: Can be split into rational or emotional sentiment. Hence resulting in the common design of 5 sentiment ratings.\n",
    "    - emotional negative (-2)\n",
    "    - rational negative (-1)\n",
    "    - neutral (0): In practice, neutral often means no opinion or sentiment expressed.\n",
    "    - rational positive (+1)\n",
    "    - emotional positive (+2)\n",
    "- Sentiment Subjectivity: Subjective sentences expresses some personal feelings, views, or beliefs. They may not express any sentiment. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the original dataset also has labels of the sentiment level, polarity and subjectivity by the original author. \n",
    "We will build our own model then compare the values to the values in the original dataset for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60037                                    Most time respond\n",
       "11065                                                  NaN\n",
       "10015    I solved problem know login app. I know hard i...\n",
       "10395                                                  NaN\n",
       "13682                                                  NaN\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_reviews = df[\"Review\"].copy()\n",
    "raw_reviews.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop NA types in the reviews data, since they do not contain any text. They probably got into the dataset because of imperfect data scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old data frame length: 64295 \n",
      "New data frame length: 37427 \n",
      "Number of rows with at least 1 NA value:  26868\n"
     ]
    }
   ],
   "source": [
    "reviews = raw_reviews.dropna(axis=0)\n",
    "print(\"Old data frame length:\", len(raw_reviews), \"\\nNew data frame length:\",  \n",
    "       len(reviews), \"\\nNumber of rows with at least 1 NA value: \", \n",
    "       (len(raw_reviews)-len(reviews))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the preprocessing steps that we will be doing for every review.\n",
    "1. Converting text to lowercase\n",
    "2. Remove html tags\n",
    "3. Expand Contractions\n",
    "    - Dictionary source from : https://github.com/pemagrg1/Text-Pre-Processing-in-Python/blob/master/individual_python_files/contractions.py\n",
    "4. Remove punctuation\n",
    "5. Using NLKT's `PunktSentenceTokenizer` to create tokens\n",
    "6. Word Normalization for Tokens\n",
    "    - Correcting mispelled words\n",
    "    - Stop word removal\n",
    "    - Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopword = set(stopwords.words('english'))\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex Patterns\n",
    "html_pattern = re.compile('<[^<]+?>')\n",
    "lengthening_pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(s):\n",
    "    global html_pattern\n",
    "    return html_pattern.sub('', s)\n",
    "    \n",
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))    \n",
    "\n",
    "def expand_contractions(text):\n",
    "    global contractions_pattern\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "            if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def reduce_lengthening(s):\n",
    "    global lengthening_pattern\n",
    "    return lengthening_pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "def lemmatize(w):\n",
    "    global lemmatizer\n",
    "    return lemmatizer.lemmatize(w)\n",
    "\n",
    "def normalize(s):\n",
    "    global spell\n",
    "    tokens = word_tokenize(s)\n",
    "    tokens = [lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def preprocess(s):\n",
    "    s = s.lower()\n",
    "    s = remove_html_tags(s)\n",
    "    s = expand_contractions(s)\n",
    "    s = remove_punctuation(s)\n",
    "    s = reduce_lengthening(s)\n",
    "    words = normalize(s)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing preprocessing on some reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sample\n",
      "5061     Great app, please make picture Airbnb trip/fla...\n",
      "49977    If encounter technical issues addressed. Very ...\n",
      "1473     Why I wait opponent I'm ready shoot???????? Al...\n",
      "36004    Why permissions for? Why would child's need th...\n",
      "54725                            Good product. Good value.\n",
      "Name: Review, dtype: object \n",
      "\n",
      "Normalized Tokens produced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5061     [great, app, please, make, picture, airbnb, tr...\n",
       "49977    [if, encounter, technical, issue, addressed, v...\n",
       "1473     [why, i, wait, opponent, ready, shoot, also, n...\n",
       "36004    [why, permission, for, why, would, child, need...\n",
       "54725                         [good, product, good, value]\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample = reviews.sample(5)\n",
    "print(\"Random Sample\")\n",
    "print(random_sample, \"\\n\")\n",
    "print(\"Normalized Tokens produced\")\n",
    "random_sample.progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying preprocessing to all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokens = reviews.apply(preprocess)\n",
    "processed_df = pd.DataFrame(reviews)\n",
    "processed_df[\"Tokens\"] = tokens\n",
    "processed_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving this data for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv(\"../data/tokenized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the most common tokens and saving the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,4))\n",
    "plt.gcf().subplots_adjust(bottom=0.15)\n",
    "\n",
    "word_counts = list(itertools.chain(*tokens))\n",
    "freq_dist = FreqDist(word_counts)\n",
    "freq_dist.plot(30, cumulative=False)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('images/freqDist.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some steps that we did not include for preprocessing that could affect our model:\n",
    "- Removing numbers: sometimes in reviews we see that users explain their rationale of giving a specific number of star rating, so we did not want to remove them.\n",
    "- Removing chinese reviews and apps: We assumed that China does not have access to Google Playstore, so there will be lesser chinese reviews and apps made in Chinese, but this is certainly not true. However, filtering them out is a hassle atm so we did not do this step.\n",
    "- Spell Checking every token: We tried this, but it causes the computation to be taking too long! So we ignore it for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `VADER SentimentIntensityAnalyzer` for calculating polarity scores for each review. The scoring is computed as following from https://github.com/cjhutto/vaderSentiment:\n",
    "\n",
    "- The `compound` score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "    - positive sentiment: compound score >= 0.05\n",
    "    - neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    - negative sentiment: compound score <= -0.05\n",
    "\n",
    "The `pos`, `neu`, and `neg` scores are ratios for proportions of text that fall in each category (so these should all add up to be 1... or close to it with float operation). These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def sentiment_polarity(s):\n",
    "    global analyzer\n",
    "    polarity_scores = analyzer.polarity_scores(s)\n",
    "    compound_score = polarity_scores[\"compound\"]\n",
    "    if compound_score >= 0.5:\n",
    "        label = \"Positive\"\n",
    "    elif compound_score > -0.05 and compound_score < 0.05:\n",
    "        label = \"Neutral\"\n",
    "    else:\n",
    "        label = \"Negative\"\n",
    "    return label, polarity_scores[\"neu\"], polarity_scores[\"pos\"], polarity_scores[\"neg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = processed_df\n",
    "df[\"Sentiment\"], df[\"Neutral Proportion\"], df[\"Positive Proportion\"], df[\"Negative Proportion\"] =  zip(*df[\"Review\"].apply(sentiment_polarity))\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/polarity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will classify the reviews based on subjectivity using a `NaiveBayesClassifier`. First, we split the data for testing and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(0.2*len(reviews))\n",
    "x_train = df[:test_size]\n",
    "x_test = df[test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a vocabulary consisting of all the words that the training set of reviews have. From the output, you can see that there is still much to be improved upon for the tokens that were obtained despite the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(word_counts)\n",
    "print(\"Number of words in vocabulary: {}\".format(len(vocabulary)))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each data point (tokens with the pos/neg tag), we will build a dictionary that says whether it has particular features/ words from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(row):\n",
    "    return ({i:(i in row[\"Review\"]) for i in vocabulary}, row[\"Sentiment\"])\n",
    "\n",
    "features = x_train.progress_apply(make_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
